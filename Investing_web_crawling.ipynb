{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import os\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "csv_path = 'D:/csv/hist_news/hist_news_data.csv'\n",
    "\n",
    "# 파일이 존재하지 않거나 비어 있으면 헤더를 추가합니다.\n",
    "if not os.path.exists(csv_path) or os.stat(csv_path).st_size == 0:\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Header', 'Content'])  # 헤더 작성\n",
    "\n",
    "def fetch_news(page):\n",
    "    results = []\n",
    "    with requests.Session() as session:\n",
    "        url = f'https://www.investing.com/news/cryptocurrency-news/{page}'\n",
    "        response = session.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        news_links = soup.select('#leftColumn div.textDiv a.title')\n",
    "        \n",
    "        for link in news_links:\n",
    "            news_url = f\"https://www.investing.com{link['href']}\"\n",
    "            news_response = session.get(news_url, headers=headers)\n",
    "            news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "            \n",
    "            title_element = news_soup.select_one(\"#leftColumn > h1\")\n",
    "            if title_element:\n",
    "                header = title_element.text.strip()\n",
    "                article_content = news_soup.select_one('#leftColumn > div.WYSIWYG.articlePage')\n",
    "                if article_content:\n",
    "                    exclude_elements = article_content.select('#imgCarousel > span, div.relatedInstrumentsWrapper > div')\n",
    "                    for exclude_element in exclude_elements:\n",
    "                        exclude_element.decompose()\n",
    "                \n",
    "                date_info = news_soup.select_one(\"#leftColumn > div:nth-child(6) > span:nth-child(1)\")\n",
    "                if date_info and article_content:\n",
    "                    temp_time = datetime.strptime(date_info.text.strip().replace(\"Published \", \"\").replace(\" ET\", \"\"), '%b %d, %Y %H:%M%p')\n",
    "                    temp_article = article_content.text.strip().replace(\",\", \" \").replace(\"\\n\", \" \")\n",
    "                    \n",
    "                    results.append([temp_time, header, temp_article])\n",
    "    return results\n",
    "\n",
    "# 병렬 처리를 위한 ThreadPoolExecutor 사용\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(fetch_news, page) for page in range(1, 2)]\n",
    "    results = [f.result() for f in futures if f.result()]\n",
    "\n",
    "# 결과를 데이터프레임으로 변환 후 CSV 파일로 저장\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "df = pd.DataFrame(flattened_results, columns=['Date', 'Header', 'Content'])\n",
    "df.to_csv(csv_path, index=False, mode='a', header=False)  # 새로운 데이터만 추가\n",
    "\n",
    "print(\"데이터 수집 및 저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
